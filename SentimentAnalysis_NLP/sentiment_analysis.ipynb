{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a Machine Learning model capable of detecting the sentimental connotation associated with a specific review. This type of analysis can be relevant to automatically understanding wether the customers are happy with a specific product or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create the dataset\n",
    "\n",
    "The data used to train the model correponds to reviews of eletronic devices bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the reviews\n",
    "\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "positive_reviews = [i.contents[0] for i in positive_reviews]\n",
    "\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "negative_reviews = [i.contents[0] for i in negative_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nI purchased this unit due to frequent blacko...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nI ordered 3 APC Back-UPS ES 500s on the reco...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nWish the unit had a separate online/offline ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nCheaper than thick CD cases and less prone t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nHi\\n\\nI brought 256 MB Kingston SD card from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  \\nI purchased this unit due to frequent blacko...          1\n",
       "1  \\nI ordered 3 APC Back-UPS ES 500s on the reco...          1\n",
       "2  \\nWish the unit had a separate online/offline ...          1\n",
       "3  \\nCheaper than thick CD cases and less prone t...          1\n",
       "4  \\nHi\\n\\nI brought 256 MB Kingston SD card from...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'Text': positive_reviews + negative_reviews, 'Sentiment': [1] * len(positive_reviews) + [0] * len(negative_reviews)}\n",
    "df = pd.DataFrame(data = data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Review: \n",
      "I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\n",
      "\n",
      "I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\n",
      "\n",
      "As always, Amazon had it to me in <2 business days\n",
      "\n",
      "\n",
      "Negative Review: \n",
      "I bought this for easy transfer of pictures from my digital camera with SD memory card anywhere not home and sometimes from other peoples memory card (xD and memory stick)..\n",
      "\n",
      "First of all I was disappointed with the flimsy, plastic design and the size of it. But it would have been ok if it worked!..IT DOESNT READ my SD card. And as menetioned in other people's review hard to insert and take out the cards! I'm scared if the cards get scratch and ruined whenever i do it. I wish I have bought this after reading amazon reviews...it's useless now. I'm lost how I can get sd card work on this and if I do, scared of frequent use for the flimsy design\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example of a positive review\n",
    "print(\"Positive Review:\", df.iloc[0]['Text'])\n",
    "\n",
    "#Example of a negative review\n",
    "print(\"\\nNegative Review:\", df.iloc[-1]['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Preprocess of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into Train and Test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment'], test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert each review in an array with the number of occurences of each word\n",
    "\n",
    "count_vect_model = CountVectorizer(decode_error='ignore', stop_words='english')\n",
    "count_vect_model.fit(X_train)\n",
    "\n",
    "X_train_cnt = count_vect_model.transform(X_train)\n",
    "X_test_cnt = count_vect_model.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test a Machine Learning model to predict whether a review has a positive or negative conotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielazevedo/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_cnt, y_train)\n",
    "\n",
    "print(\"Test Accuracy:\", round(model.score(X_test_cnt, y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "armband -0.871\n",
      "bad -0.949\n",
      "best 1.207\n",
      "bit 0.865\n",
      "clear 0.855\n",
      "disappointed -0.804\n",
      "easy 1.162\n",
      "excellent 1.427\n",
      "extra 0.985\n",
      "fast 0.809\n",
      "great 1.401\n",
      "highly 1.198\n",
      "item -1.026\n",
      "live -1.039\n",
      "memory 0.875\n",
      "months -0.927\n",
      "perfect 1.173\n",
      "perfectly 1.135\n",
      "poor -1.196\n",
      "pretty 0.876\n",
      "price 1.149\n",
      "refund -0.871\n",
      "return -1.287\n",
      "returned -1.203\n",
      "returning -0.968\n",
      "terrible -1.154\n",
      "unless -0.866\n",
      "waste -1.063\n"
     ]
    }
   ],
   "source": [
    "#Analysis of the words with bigger impact in the prediction\n",
    "\n",
    "threshold = 0.8\n",
    "for i, j in zip(count_vect_model.get_feature_names(), model.coef_[0]):\n",
    "    if j > threshold or j < -threshold:\n",
    "        print(i, round(j, 3))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of **80%** was obtained in predicting the sentimental conotation associated with a review. \n",
    "\n",
    "As we could expect, words like *excellent*, *fast* and *perfect* have a positive impact in the prediction, meaning that they are related to a positive review. Opposingly, words like *bad*, *poor* and *return* are associated with a bad review, given their negative impact for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditional Step - Filter words in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '002',\n",
       " '007radardetectors',\n",
       " '00ghz',\n",
       " '01',\n",
       " '0183',\n",
       " '05',\n",
       " '06',\n",
       " '09',\n",
       " '0_20',\n",
       " '0gb',\n",
       " '0s',\n",
       " '10',\n",
       " '100']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_model.get_feature_names()[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are a lot of words not relevant for prediction like *04*, *05* and *0s*. Also, there are a lot of words that have the same basic form, like *return*, *returned* and *returning*. Next, we will try to remove these redundancies using **stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielazevedo/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "\n",
    "\n",
    "#process sentences, by extracting only relevant words. Numbers, punctuation and shorter words are filtered\n",
    "def stemming(s):\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) >= 3]\n",
    "    tokens = [t.translate(str.maketrans('', '', string.punctuation)) for t in tokens]\n",
    "    tokens = [t for t in tokens if not any(i.isdigit() for i in t)]\n",
    "    tokens = [ps.stem(t) for t in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "count_vect_model = CountVectorizer(decode_error='ignore', tokenizer = stemming)\n",
    "count_vect_model.fit(X_train)\n",
    "\n",
    "X_train_cnt = count_vect_model.transform(X_train)\n",
    "X_test_cnt = count_vect_model.transform(X_test)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_cnt, y_train)\n",
    "\n",
    "print(\"Test Accuracy:\", round(model.score(X_test_cnt, y_test), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\\x1aafter',\n",
       " '\\x1aat',\n",
       " '\\x1ain',\n",
       " '\\x1athe',\n",
       " 'a',\n",
       " 'aaa',\n",
       " 'aac',\n",
       " 'aback',\n",
       " 'aband',\n",
       " 'abandon',\n",
       " 'abc',\n",
       " 'abeit',\n",
       " 'aberr',\n",
       " 'abil']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_model.get_feature_names()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almost -1.023\n",
      "armband -0.883\n",
      "back -0.964\n",
      "best 1.157\n",
      "bit 0.848\n",
      "easi 1.076\n",
      "excel 1.162\n",
      "extra 0.849\n",
      "fast 1.048\n",
      "flaw -0.823\n",
      "great 1.314\n",
      "highli 0.962\n",
      "item -0.821\n",
      "memori 1.012\n",
      "month -0.807\n",
      "not -1.11\n",
      "perfect 1.146\n",
      "perfectli 1.134\n",
      "poor -1.325\n",
      "pretti 0.993\n",
      "price 0.919\n",
      "refund -0.82\n",
      "return -1.725\n",
      "terribl -1.203\n",
      "wast -1.168\n",
      "worth 0.989\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "for i, j in zip(count_vect_model.get_feature_names(), model.coef_[0]):\n",
    "    if j > threshold or j < -threshold:\n",
    "        print(i, round(j, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, by applying stemming in the training corpus the results improved slightly (from 0.795 to 0.81). Further validation would be needed to conclude that, indeed, stemming helps to improve the results. Nevertheless, by applying stemming some relevant information may be lost. For example, if there are two words that are similar in their writing but have different meanings, they will be treated in the same way. However, it may help remove redundancy in some words like *return* and *returned*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further work could be applying other Machine Learning techniques for improving results and trying other feature extraction methods, besides *CountVectorizer*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

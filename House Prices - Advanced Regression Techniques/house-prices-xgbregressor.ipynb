{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "#Score: 0.165\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fancyimpute import KNN, SimpleFill\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "import os\n",
    "import xgboost\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
    "    test_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "train_df, test_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_categorical(df, column_names):\n",
    "    \n",
    "    for col in column_names:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "            counters = pd.get_dummies(df[col])\n",
    "            df = pd.concat([df, pd.get_dummies(df[col],prefix=col, drop_first=True) ],axis=1)\n",
    "            df.drop([col],axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/1460 with 0 missing, elapsed time: 0.991\n",
      "Imputing row 101/1460 with 1 missing, elapsed time: 0.994\n",
      "Imputing row 201/1460 with 0 missing, elapsed time: 0.997\n",
      "Imputing row 301/1460 with 0 missing, elapsed time: 1.000\n",
      "Imputing row 401/1460 with 0 missing, elapsed time: 1.002\n",
      "Imputing row 501/1460 with 0 missing, elapsed time: 1.004\n",
      "Imputing row 601/1460 with 0 missing, elapsed time: 1.007\n",
      "Imputing row 701/1460 with 0 missing, elapsed time: 1.010\n",
      "Imputing row 801/1460 with 0 missing, elapsed time: 1.012\n",
      "Imputing row 901/1460 with 1 missing, elapsed time: 1.014\n",
      "Imputing row 1001/1460 with 5 missing, elapsed time: 1.017\n",
      "Imputing row 1101/1460 with 0 missing, elapsed time: 1.019\n",
      "Imputing row 1201/1460 with 0 missing, elapsed time: 1.022\n",
      "Imputing row 1301/1460 with 1 missing, elapsed time: 1.024\n",
      "Imputing row 1401/1460 with 0 missing, elapsed time: 1.026\n",
      "Imputing row 1/1459 with 0 missing, elapsed time: 0.924\n",
      "Imputing row 101/1459 with 5 missing, elapsed time: 0.926\n",
      "Imputing row 201/1459 with 0 missing, elapsed time: 0.928\n",
      "Imputing row 301/1459 with 0 missing, elapsed time: 0.931\n",
      "Imputing row 401/1459 with 5 missing, elapsed time: 0.935\n",
      "Imputing row 501/1459 with 0 missing, elapsed time: 0.937\n",
      "Imputing row 601/1459 with 0 missing, elapsed time: 0.939\n",
      "Imputing row 701/1459 with 0 missing, elapsed time: 0.942\n",
      "Imputing row 801/1459 with 0 missing, elapsed time: 0.946\n",
      "Imputing row 901/1459 with 0 missing, elapsed time: 0.947\n",
      "Imputing row 1001/1459 with 0 missing, elapsed time: 0.950\n",
      "Imputing row 1101/1459 with 0 missing, elapsed time: 0.952\n",
      "Imputing row 1201/1459 with 0 missing, elapsed time: 0.955\n",
      "Imputing row 1301/1459 with 0 missing, elapsed time: 0.958\n",
      "Imputing row 1401/1459 with 0 missing, elapsed time: 0.961\n",
      "Relevant Features Index(['OverallQual', 'ExterQual', 'BsmtQual', 'TotalBsmtSF', '1stFlrSF',\n",
      "       'GrLivArea', 'FullBath', 'KitchenQual', 'GarageCars', 'GarageArea'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_ids = test_df['Id'].values\n",
    "\n",
    "\n",
    "# Checking for Missing Data\n",
    "\n",
    "#remove features with Nan percentage higher than 0.45\n",
    "nan_columns = train_df.columns.values[train_df.isna().any()]\n",
    "\n",
    "for i in range(len(nan_columns)):\n",
    "    col_values = train_df.loc[:, nan_columns[i]]\n",
    "    percentage_null = round(col_values.isnull().sum()/len(col_values), 2)\n",
    "    #print(\"Nan Percentage of {}: {}\".format(nan_columns[i], percentage_null))\n",
    "\n",
    "    if round(col_values.isnull().sum()/len(col_values), 2) > 0.45:\n",
    "        train_df.drop(nan_columns[i], axis=1, inplace=True)\n",
    "        test_df.drop(nan_columns[i], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Analysis of feature data types\n",
    "\n",
    "'''\n",
    "# Nan in Garages or Basement, mean there are none\n",
    "for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "    train_df[col] = train_df[col].fillna(0)\n",
    "    test_df[col] = train_df[col].fillna(0)\n",
    "    \n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    train_df[col] = train_df[col].fillna('None')\n",
    "    test_df[col] = train_df[col].fillna('None')\n",
    "        \n",
    "for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
    "    train_df[col] = train_df[col].fillna('None')\n",
    "    test_df[col] = train_df[col].fillna('None')\n",
    "'''\n",
    "    \n",
    "   \n",
    "train_df['MSSubClass'] = train_df['MSSubClass'].astype(str)\n",
    "train_df['YrSold'] = train_df['YrSold'].astype(str)\n",
    "train_df['MoSold'] = train_df['MoSold'].astype(str)\n",
    "\n",
    "test_df['MSSubClass'] = test_df['MSSubClass'].astype(str)\n",
    "test_df['YrSold'] = test_df['YrSold'].astype(str)\n",
    "test_df['MoSold'] = test_df['MoSold'].astype(str)\n",
    "\n",
    "\n",
    "# Preprocess categorical variables\n",
    "\n",
    "#label the categorical variables\n",
    "labels_categorical = np.array([])\n",
    "labels_non_categorical = np.array([])\n",
    "for ind, col in enumerate(train_df.columns):\n",
    "    col_values = train_df.loc[:, col]\n",
    "    \n",
    "    #check for strings\n",
    "    if all(isinstance(x, (str, float)) for x in col_values) is True:\n",
    "        \n",
    "        #replacing missing values by the mode\n",
    "        #train_df[col] = train_df[col].fillna(train_df[col].mode()[0])\n",
    "        \n",
    "        #encode categorical variables\n",
    "        not_nan_vals = train_df.loc[train_df[col].notnull(), col]\n",
    "        labelencoder_X = LabelEncoder()\n",
    "        train_df.loc[train_df[col].notnull(), col] = labelencoder_X.fit_transform(not_nan_vals)\n",
    "        \n",
    "        not_nan_vals = test_df.loc[test_df[col].notnull(), col]\n",
    "        labelencoder_X = LabelEncoder()\n",
    "        test_df.loc[test_df[col].notnull(), col] = labelencoder_X.fit_transform(not_nan_vals)\n",
    "        \n",
    "        labels_categorical = np.append(labels_categorical, col)\n",
    "    else:            \n",
    "        if col != 'SalePrice':\n",
    "            labels_non_categorical = np.append(labels_non_categorical, col)\n",
    "        \n",
    "        \n",
    "\n",
    "#Fill missing values\n",
    "\n",
    "#using the most similar instance to fill the remaining fields\n",
    "train_df.loc[:,:] = KNN(k=1).fit_transform(train_df)\n",
    "test_df.loc[:,:] = KNN(k=1).fit_transform(test_df)\n",
    "\n",
    "\n",
    "#Feature selection: Correlation analysis\n",
    "\n",
    "cor = train_df.corr()\n",
    "\n",
    "#plt.figure(figsize=(12,10))\n",
    "#sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "#plt.show()\n",
    "\n",
    "cor_price = abs(cor['SalePrice'])\n",
    "relevant_features = cor_price[cor_price>0.55].index\n",
    "relevant_features = relevant_features.drop(labels = ['SalePrice'])  \n",
    "\n",
    "print('Relevant Features {}'.format(relevant_features))\n",
    "\n",
    "train_target = train_df['SalePrice']\n",
    "\n",
    "train_df = train_df.loc[:, relevant_features]\n",
    "test_df = test_df.loc[:, relevant_features]\n",
    "\n",
    "\n",
    "labels_non_categorical = [i for i in labels_non_categorical if i in relevant_features]\n",
    "\n",
    "#Normalize data\n",
    "sc = StandardScaler()\n",
    "train_df[labels_non_categorical] = sc.fit_transform(train_df[labels_non_categorical])\n",
    "test_df[labels_non_categorical] = sc.transform(test_df[labels_non_categorical])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use One Hot Encoding\n",
    "train_df = convert_to_categorical(train_df, labels_categorical)\n",
    "test_df = convert_to_categorical(test_df, labels_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/opt/conda/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:55:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "{'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "#XGBOOST\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [1, 5],\n",
    "    'n_estimators': [1000, 5000],\n",
    "    'learning_rate': [0.01, 0.05]\n",
    "}\n",
    "\n",
    "#Cross Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgboost.XGBRegressor(),\n",
    "    param_grid=parameters,\n",
    "    n_jobs = 10,\n",
    "    cv = 10\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(train_df, train_target)\n",
    "print(grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = best_model.predict(test_df)\n",
    "\n",
    "#export the results\n",
    "with open('test_results.csv', 'w') as writeFile:\n",
    "    \n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerow(['Id', 'SalePrice'])\n",
    "    \n",
    "    for ind, sample in enumerate(results):\n",
    "        writer.writerow([test_ids[ind], sample])\n",
    "\n",
    "writeFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

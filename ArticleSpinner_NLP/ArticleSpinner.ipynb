{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in the notebook is to implement an article spinner, which will replace some words of a given text by a synonym\n",
    "\n",
    "Code partly adapted from: https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent the given text in a 4-gram\n",
    "# (w1, PosTag(w2), w3) is the key, [ w2 ] are the values\n",
    "# w1 represent past word, w2 the current word and w3 the future word\n",
    "\n",
    "def lang_model(corpus):\n",
    "    \n",
    "    ngram = {}\n",
    "    for review in corpus:\n",
    "        s = review.text.lower()\n",
    "        tokens = nltk.tokenize.word_tokenize(s)\n",
    "        \n",
    "        for i in range(len(tokens) - 2):\n",
    "            k = (tokens[i], nltk.pos_tag([tokens[i+1]])[0][1], tokens[i+2])\n",
    "            if k not in ngram:\n",
    "                ngram[k] = []\n",
    "            ngram[k].append(tokens[i+1])\n",
    "    \n",
    "    # turn each array of middle-words into a probability vector\n",
    "    ngram_prob = {}\n",
    "    for k, words in ngram.items():\n",
    "\n",
    "        if len(set(words)) > 1:\n",
    "            d = {}\n",
    "            n = 0\n",
    "            for w in words:\n",
    "                if w not in d:\n",
    "                    d[w] = 0\n",
    "                d[w] += 1\n",
    "                n += 1\n",
    "            for w, c in d.items():\n",
    "                d[w] = float(c) / n\n",
    "            ngram_prob[k] = d\n",
    "            \n",
    "    return ngram_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a element of d_probs, based on the given probabilities\n",
    "def random_sample(d_probs):\n",
    "    r = random.random()\n",
    "    cumulative = 0\n",
    "    for w, p in d_probs.items():\n",
    "        cumulative += p\n",
    "        if r < cumulative:\n",
    "            return w\n",
    "\n",
    "\n",
    "def test_spinner(reviews, ngram):\n",
    "    \n",
    "    chosen_prob = 1\n",
    "    s = random.choice(reviews).text.lower()\n",
    "    \n",
    "    print(\"Original Text:\", s)\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    for i in range(len(tokens) - 2):\n",
    "        if random.random() < chosen_prob:\n",
    "            k = (tokens[i], nltk.pos_tag([tokens[i+1]])[0][1], tokens[i+2])\n",
    "            if k in ngram:\n",
    "                w = random_sample(ngram[k])\n",
    "                tokens[i+1] = w\n",
    "    print(\"Updated Text:\")\n",
    "    print(\" \".join(tokens).replace(\" .\", \".\").replace(\" '\", \"'\").replace(\" ,\", \",\").replace(\"$ \", \"$\").replace(\" !\", \"!\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: \n",
      "i like it very much, i use it in an outdoor trail camera for wild game and it hold several pictures and has been in the camera in all kinds of weather. i would buy kingston again\n",
      "\n",
      "Updated Text:\n",
      "i like them very small, everyday use it in an outdoor trail camera for wild game and it hold several months and has been for the device in all kinds of sink. i'd buy kingston again\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read(), \"lxml\")\n",
    "reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "\n",
    "ngram = lang_model(reviews)\n",
    "\n",
    "test_spinner(reviews, ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, the accuracy of the model is not great as some of the replacements don't make sense in the given context. This should be due to the assumed Markov assumption in the creation of the n-gram. Still, this creation of Article Spinners correspond to a real case scenario where NLP can be applied as a valid contribute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
